#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Nov 22 18:04:25 2022

@author: tom
"""

import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split 
from sklearn import metrics
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

rng = np.random.RandomState(60)
x = 10 * rng.rand(120)
y = 2 * x - 1 + rng.randn(120)
plt.scatter(x, y);

model = LinearRegression(fit_intercept=True)
print(model)

X = x[:, np.newaxis]
X.shape
print(X.shape)

model.fit(X, y)

# The gradient (coefficient):
print(model.coef_)
# The y-intercept:
print(model.intercept_)

# Next, testing the model by feeding it a random grid of x values:
xfit = np.linspace(-1, 11)

# These x values are then coerced into an [n samples, n features]
# features matrix 
Xfit = xfit[:, np.newaxis]
yfit = model.predict(Xfit)

# Visualise the data:
plt.scatter(x, y)
plt.plot(xfit, yfit);

path = "../ex1/" 

filename_read = os.path.join(path, "auto-mpg.csv")
df = pd.read_csv('/Users/tom/Documents/AI Course/INM701 - Intro/Masters-Module-1/Week 3/auto-mpg.csv', na_values=['NA', '?'])

print(df[:5])

# Checking for null data, presumably for at least one data point in a class:
print(df.isnull().any())

# Replacing null values with the class median:
med = df['horsepower'].median()
df['horsepower'] = df['horsepower'].fillna(med)
#df = df.dropna() #you can also simply drop NA values

# Checking to make sure the null values are gone: 
print(df.isnull().any())

# Strip non-numeric features from the dataframe
df = df.select_dtypes(include=['int', 'float'])

#print to check that this has worked
print(df[:5]) 

# Below we are predicting mpg values based on other numeric values I think. 
result = []
for x in df.columns:
    if x != 'mpg':
        result.append(x)
   
X = df[result].values
y = df['mpg'].values

#split data into testing and training
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# build the model
model = LinearRegression()  
model.fit(X_train, y_train)

print(model.coef_)

#calculate the predictions of the linear regression model
y_pred = model.predict(X_test)

#build a new data frame with two columns, the actual values of the test data, 
#and the predictions of the model
df_compare = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
df_head = df_compare.head(25)
print(df_head)

# Plotting the actual vs predicted 'mpg' values, and doing some stats:
    
df_head.plot(kind='bar',figsize=(10,8))
plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
plt.show()

print('Mean:', np.mean(y_test))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

# No hard rule for RMSE but it's above 10% of mean so might want 
# to improve data with normalisation or ommitting some features from
# regression model. 
# Basically try and improve the model. 

# Regression chart.
def chart_regression(pred, y, sort=True):
    t = pd.DataFrame({'pred': pred, 'y': y.flatten()})
    if sort:
        t.sort_values(by=['y'], inplace=True)
    plt.plot(t['y'].tolist(), label='expected')
    plt.plot(t['pred'].tolist(), label='prediction')
    plt.ylabel('output')
    plt.legend()
    plt.show()
    
chart_regression(y_pred[:100].flatten(),y_test[:100],sort=True)  

# PART 2 - Support Vector Machines (SVMs)


from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=120, centers=2,
                  random_state=0, cluster_std=0.60)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='winter');

# Drawing a hyperplane to separate data values:
    
#xfit = np.linspace(-1, 3.8)
#plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
#plt.plot([0.6], [2.1], 'x', color='orange', markeredgewidth=2, markersize=10)

#for m, b in [(0.8, 1.0), (0.5, 1.6), (-0.3, 2.9)]:
    # y = mx + c
#    plt.plot(xfit, m * xfit + b, '-k')

plt.xlim(-1, 3.8);

# Drawing a margin with width = distance to nearest point:
    
xfit = np.linspace(-1, 3.5)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')

for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:
    yfit = m * xfit + b
    plt.plot(xfit, yfit, '-k')
    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',
                     color='#AAAAAA', alpha=0.4)

plt.xlim(-1, 3.5);

# Maximum margin is best. This is example of max. margin estimator. 


from sklearn.svm import SVC # "Support vector classifier"
model = SVC(kernel='linear', C=1E10)
model.fit(X, y)

def plot_svc_decision_function(model, ax=None, plot_support=True):
    """Plot the decision function for a 2D SVC"""
    if ax is None:
        ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()
    
    # create grid to evaluate model
    x = np.linspace(xlim[0], xlim[1], 30)
    y = np.linspace(ylim[0], ylim[1], 30)
    Y, X = np.meshgrid(y, x)
    xy = np.vstack([X.ravel(), Y.ravel()]).T
    P = model.decision_function(xy).reshape(X.shape)
    
    # plot decision boundary and margins
    ax.contour(X, Y, P, colors='k',
               levels=[-1, 0, 1], alpha=0.5,
               linestyles=['--', '-', '--'])
    
    # plot support vectors
    if plot_support:
        ax.scatter(model.support_vectors_[:, 0],
                   model.support_vectors_[:, 1],
                   s=300, linewidth=1, facecolors='none');
    ax.set_xlim(xlim)
    ax.set_ylim(ylim)
    
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plot_svc_decision_function(model);

# Support vectors are points that are close to the hyperplane,
# they give SVM its name. 

print("Support vectors for SVM: ", "\n", model.support_vectors_, "\n")

# SVM (hyperplanes) only change when the SVs change, so even if you
# the number of data points, the lines wouldnt change if the SVs were constant.

def plot_svm(N=10, ax=None):
    X, y = make_blobs(n_samples=200, centers=2,
                      random_state=0, cluster_std=0.60)
    X = X[:N]
    y = y[:N]
    model = SVC(kernel='linear', C=1E10)
    model.fit(X, y)
    
    ax = ax or plt.gca()
    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
    ax.set_xlim(-1, 4)
    ax.set_ylim(-1, 6)
    plot_svc_decision_function(model, ax)

fig, ax = plt.subplots(1, 2, figsize=(16, 6))
fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)
for axi, N in zip(ax, [60, 120]):
    plot_svm(N, axi)
    axi.set_title('N = {0}'.format(N))


from ipywidgets import interact, fixed
interact(plot_svm, N=[10, 20, 40, 80, 200], ax=fixed(None));


# This data can't be separated by a line, and so we need to use a kernel. 

from sklearn.datasets import make_circles
X, y = make_circles(100, factor=.1, noise=.1)

clf = SVC(kernel='linear').fit(X, y)

plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plot_svc_decision_function(clf, plot_support=False);


# Radial basis function around central cluster:
r = np.exp(-(X ** 2).sum(1))

# from mpl_toolkits import mplot3d

def plot_3D(elev=-90, azim=30, X=X, y=y):
    ax = plt.subplot(projection='3d')
    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')
    ax.view_init(elev=elev, azim=azim)
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_zlabel('r')

interact(plot_3D, elev=(-90, 90), azip=(-180, 180),
         X=fixed(X), y=fixed(y));

# See 'kernel trick' 
# it is based on a similarity relationship (or kernel) between 
# each pair of points.

clf = SVC(kernel='rbf', C=1E6, gamma='auto')
clf.fit(X, y)

# We have turned a fast linear problem into a fast non-linear problem. 













































